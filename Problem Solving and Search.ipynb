{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d61a620-f26c-4b0a-b44e-dea8ff7dd14c",
   "metadata": {},
   "source": [
    "### Things from the previous chapter that we need to know \n",
    "1. Agent- agent perceives environment using perceptors and acts upon it by using actuators. Mathematically speaking, agent’s behavior is described by the agent function that maps any given percept sequence to an action. The agent function is an abstract mathematical description; the agent program is a concrete implementation, running within some physical system.\n",
    "2. Percept Sequence- complete history of everything the agent has ever perceived\n",
    "3. When we define a problem we should definitely define the task environment of the agent. There are usually 2 types of environment\n",
    "    1. fully visible- If an agent’s sensors give it access to the complete state of the environment at each point in time, then we say that the task environment is fully observable.\n",
    "    2. partially visible- An environment might be partially observable because of noisy and inaccurate sensors or because parts of the state are simply missing from the sensor data.\n",
    "4. There are several other classifications for an environment\n",
    "    1. If the next state of the environment is completely determined by the current state and the action executed by the agent, then we say the environment is deterministic; otherwise, it is stochastic.\n",
    "    2. In episodic vs sequential environments- in episodic environemnt, action based on one percept would not define the outcone of the next percept but in a seuqential environemnt, action of one percept will affect all the future decisions\n",
    "    3. If the environment can change while an agent is deliberating, then we say the environment is dynamic for that agent; otherwise, it is static. Unlike static environemnet, in dynamic environments you need to deal with the passage of time. If the environment itself does not change with the passage of time but the agent’s performance score does, then we say the environment is semidynamic. This might happen because a dynamic environment is always asking an agent what is the next thing that it wants to do and if the agent does not answer, it is perceived as the agent does not want to do anything\n",
    "    4. Discrete systems: the state change only at a countable number of points in time. These points in time are the ones at which the event occurs/change in state. Continuous: the state variables change in a continuous way, and not abruptly from one state to another (infinite number of states).\n",
    "    5. In a known environment, the outcomes (or outcome probabilities if the environment is stochastic) for all actions are given. If the environment is unknown, the agent will have to learn how it works in order to make good decisions. This distinction refers to the agent's knowledge of the physics of the environment more than the visibility of the environment\n",
    "5. Types of agents based on decision making paradigms\n",
    "    1. Simple Reflex Agents- the type of agents that use only current percept to take decision and not the percept history\n",
    "    2. Model based Reflex Agents- If the agent cannot fully observe the system at all states, it should maintain an internal variable that keeps memory of the percept history to increase such visibility. But updating this internal state information requires some kind of understanding of the evolution of the world independent of the agent and also how the agen't own action affect the world\n",
    "    3. Goal Based Ageents- A goal-based agent is an artificial intelligence agent that responds to its environment and adjusts accordingly to achieve a goal.\n",
    "    4. Utility Based Agents- Utility-based agents are designed to find the best solution based on a specific utility. They use optimization algorithms to find the best solution. To understand the difference from goal based agents\n",
    "        1. Goals: Utility-based agents don't have a particular goal. Goal-based agents act in order to achieve their goal(s).\n",
    "        2. Solutions: Utility-based agents use optimization algorithms to find the best solution. Goal-based agents take uncertain paths, some of which are less efficient than others.\n",
    "        3. Utility measurement: Utility-based agents provide a measure of success at a given state. \n",
    "    6. Learning Agents. This can be divided into 4 conceptual components.\n",
    "        1. The learning element learns and makes improvements\n",
    "        2. The performance element selects external actions (which is what we have been assuming to be the agent until now)\n",
    "        3. The critic element provides feedback to the learning element on how it is doing so far wrt a fixed performance standard\n",
    "        4. the problem generator- The point is that if the performance element had its way, it would keep doing the actions that are best, given what it knows. But if the agent is willing to explore a little and do some perhaps suboptimal actions in the short run, it might discover much better actions for the long run. The problem generator’s job is to suggest these exploratory actions hence taking into account and keeping in check the exploration vs exploitation trade off in agent based systems  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8e46ce-48b6-43c1-a9d5-d5f459b841df",
   "metadata": {},
   "source": [
    "\n",
    "## Problem Definition \n",
    "Components of a problem \n",
    "1. initial state where the agent starts\n",
    "2. A description of the possible actions available to the agent. Given a particular state s, ACTIONS(s) returns the set of actions that can be executed in s.\n",
    "3. A description of what each action does; the formal name for this is the transition model, specified by a function RESULT(s,a) that returns the state that results from doing action a in state s. We also use the term successor to refer to any state reachable from a given state by a single action. Together, the initial state, actions, and transition model implicitly define the state space of the problem\n",
    "4. The goal test, which determines whether a given state is a goal state.\n",
    "5. A path cost function that assigns a numeric cost to each path. The problem-solving agent chooses a cost function that reflects its own performance measure. The step cost of taking action a in state s to reach state s’ is denoted by c(s,a,s’).\n",
    "6. A solution to a problem is a sequence of steps that leads us to the goal state\n",
    "\n",
    "\n",
    "The process of removing detail from a representation ABSTRACTION is called abstraction. In order to solve mathematically dense problems, we need to abstract our problem definition "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e6adb1-ef8e-4213-a047-47aee01df435",
   "metadata": {},
   "source": [
    "## Defining some popular toy problems in the world of AI \n",
    "1. In the classical vacuum cleaner problem, we have two rooms and one vacuum cleaner. There is dirt in both the rooms and it is to be cleaned.he vacuum cleaner is present in any one of these rooms. So, we have to reach a state in which both the rooms are clean and are dust free. The vaccum cleaner is the agent, the available actions are left / right and suck. We can assume that each path costs us 1\n",
    "2. The 8 puzzle problem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd11b57-8ab7-4d7a-9e40-7f3cdf87a026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
